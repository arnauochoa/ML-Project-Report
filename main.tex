\documentclass[a4paper, report, oneside, UKenglish]{memoir}


%% Title page
\usepackage{projectfp}


%% Silence warning about obsolete package
\usepackage{silence}
\WarningFilter{remreset}{The remreset package}


%% Encoding
\usepackage[utf8]{inputenx} % Source code
\usepackage[T1]{fontenc}    % PDF


%% Fonts and typography
\usepackage{lmodern}           % Latin Modern Roman
\usepackage[scaled]{beramono}  % Bera Mono (Bitstream Vera Sans Mono)
\renewcommand{\sfdefault}{lmss} % Latin Modern Sans Serif
\usepackage[final]{microtype}  % Improved typography
\renewcommand{\abstractnamefont}{\Large\sffamily\bfseries}           % Abstract
\renewcommand*{\chaptitlefont}{\Huge\bfseries\rmfamily\raggedright}  % Chapter
\setsecheadstyle{\Large\bfseries\sffamily\raggedright}               % Section
\setsubsecheadstyle{\large\bfseries\sffamily\raggedright}            % Subsection
\setsubsubsecheadstyle{\normalsize\bfseries\sffamily\raggedright}    % Subsubsection
\setparaheadstyle{\normalsize\bfseries\sffamily\raggedright}         % Paragraph
\setsubparaheadstyle{\normalsize\bfseries\sffamily\raggedright}      % Subparagraph
\setsecnumdepth{subsection}

\usepackage{geometry}

%% Mathematics
\usepackage{amsmath}
\usepackage{amssymb}   % Extra symbols
\usepackage{amsthm}    % Theorem-like environments
\usepackage{thmtools}  % Theorem-like environments
\usepackage{mathtools} % Fonts and environments for mathematical formulae
\usepackage{mathrsfs}  % Script font with \mathscr{}
\usepackage{physics}

%% Miscellanous
\usepackage{graphicx}  % Tool for images
\graphicspath{{figures/}}
\usepackage{babel}     % Automatic translations
\usepackage{csquotes}  % Quotes
\usepackage{textcomp}  % Extra symbols
\usepackage{listings}  % Typesetting code
\lstset{basicstyle = \ttfamily, frame = tb}
\usepackage{multirow}  % Multirow equations
\usepackage[table,xcdraw]{xcolor} % Color for tables
\usepackage{gensymb}   % Symbols
\usepackage[center]{caption}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref} % Hide red boxes around links


%% Bibliography
\usepackage[style=ieee]{biblatex}
\usepackage{mathscinet}
% \usepackage[backend    = biber,
%             sortcites  = true,
%             giveninits = true,
%             doi        = false,
%             isbn       = false,
%             url        = false,
%             style      = alphabetic
%             citestyle  = numeric]{biblatex}
% \DeclareNameAlias{sortname}{family-given}
% \DeclareNameAlias{default}{family-given}
% \DeclareFieldFormat[article]{volume}{\bibstring{jourvol}\addnbspace#1}
% \DeclareFieldFormat[article]{number}{\bibstring{number}\addnbspace#1}
% \renewbibmacro*{volume+number+eid}
% {
%     \printfield{volume}
%     \setunit{\addcomma\space}
%     \printfield{number}
%     \setunit{\addcomma\space}
%     \printfield{eid}
% }
\bibliography{bibliography}
\addbibresource{bibliography.bib}

%% Cross references
%\usepackage{varioref}
%\usepackage[pdfusetitle]{hyperref}
%\urlstyle{sf}
%\usepackage[nameinlink, capitalize, noabbrev]{cleveref}
%\crefname{chapter}{Section}{Sections}

%% Delimiters
\DeclarePairedDelimiter{\paren}{\lparen}{\rparen}   % Parenthesis
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace} % Set
% \DeclarePairedDelimiter{\abs}{\lvert}{\rvert}   % Absolute value
% \DeclarePairedDelimiter{\norm}{\lVert}{\rVert}  % Norm


%% Operators
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\im}{im}
% \DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

%% New commands for vectors
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}


%% New commands for sets
\newcommand{\N}{\mathbb{N}}   % Natural numbers
\newcommand{\Z}{\mathbb{Z}}   % Integers
\newcommand{\Q}{\mathbb{Q}}   % Rational numbers
\newcommand{\R}{\mathbb{R}}   % Real numbers
\newcommand{\C}{\mathbb{C}}   % Complex numbers
\newcommand{\A}{\mathbb{A}}   % Affine space
\renewcommand{\P}{\mathbb{P}} % Projective space


%% Miscellanous
\renewcommand{\qedsymbol}{\(\blacksquare\)}
\setlength{\parindent}{0pt} % Indent before each paragraph
\setlength{\parskip}{7pt}   % Space between paragraphs


\title{ML Project}
\author{Andrea Bellés Ferreres \newline Arnau Ochoa Bañuelos}
\supervisor{Carl Milner\newline Heekwon No}

\begin{document}

\projectfrontpage

\newpage
\begin{abstract}
\noindent

\end{abstract}

\newpage
\begin{KeepFromToc}
    \tableofcontents
\end{KeepFromToc}

\newpage
\chapter{Introduction}\label{ch:intro}
Global Navigation Satellite System (GNSS) refers to a constellation of satellites transmitting signals from space that provide positioning and timing data to GNSS receivers. Then, these receivers use the data to determine the their own location.

Satellite-based systems present many advantages with respect to other positioning systems, such that global coverage and long-term accuracy with errors limited to a few meters \cite{groves}, considering a stand-alone standard GNSS receiver. Consequently, this type of navigation systems are being employed in a wide range of applications, not only for positioning purposes, but also to improve traffic flow, assist in civil protection and rescue operations, and research on subjects such that geodesy, meteorology and surveying, among many others \cite{EGSAwebsite}. 

The rising demand of real-time information entiles the use of GNSS in many fields, such as the European railway network. In December 2019, the European Commission invested in the development of a new research project, known as Certifiable Localisation Unit with GNSS  (CLUG) in the railway environment, within the framework of the Horizon 2020 programme. This EU project aims to assess the feasibility of a failsafe on-board multi-sensor localisation unit using GNSS, track maps and a minimal number of reference points \cite{CORDISwebsite}. It will also review the use of an on-board continuous localisation system that provides information on location, speed and other dynamics of the train \cite{CORDISwebsite}. The future purpose is the development of train digitalisation and automation. 

In addition to the coordinate collaboration of multiple railway companies, railway signalling industries and navigation specialists \cite{CORDISwebsite}, part of this project is carried out in the École Nationale de l'Aviation Civile (ENAC). It is proposing to develop an innovate model of the local errors impacting the GNSS receiver. Besides that, ENAC's role consists in helping to develop the end-to-end integrity concept including an assesment of the error and fault models.  

In order to support this work, ENAC is collecting its own data within the complex urban environments that the CLUG project is addressing by using the "Jumpy" research vehicle of the institution.

As part of the work developed by ENAC, this project aims to contribute to the development of different regression techniques to generate a model for the local errors. Different fitting algorithms have been implemented for this modelling: a Matlab fitting function, Gradient Descent (GD) and Neural Networks (NN). Each of them will be further detailed in the following chapters. 

% search references of the use of neural networks for estimating the quality of the signal (multipath, overall error, etc)

%%% todo: resume of what are we going to do mentioning where it is going to be explain:
%%%% Ex: Throughout this document we will analyse.... blablabla in chapter 1.... sth like this
%%Throughout this project, regression models have been considered in order to solve a predictive problem with regard to the modelling of local errors present in a set of features, which have been provided by the supervisors of this project, and the corresponding polynomial relationship of those features which best captures the error distribution

% s'han desenvolupat els metodes fent servir MATLAB fitting, pero despres el analisis principal es amb mes features. Mes que res perque 
% 1. sabem (o intuim bastant) que modelar el mp amb cno i elev es dificil, pq no ens donen prou info
% 2. la gracia de fer servir NN es poder analitzar mes features de les que analitzaries amb altres metodes (i trobar patrons mes complexos), del que ho faries amb un LS o GD. (a part del aprenentatge automatic i tal) 
% crec que aquest es com el main purpose del treball, rollo la hipotesi que tenim i tal

        
% ...

% Fitting (o regression o whatever)
%     matlab fitting -> first reference
%     gradient descent -> interesting method (?), reference for NN
%     neural networks -> expected to have better performance since it allows to find more complex relations between data (aprox)

%==================================================================================================================
\chapter{Data analysis}\label{ch:data_analysis}
%**** explain the concept of features

% Introduction
As it has been previously explained, the purpose of this project is to develop a regression method capable of predicting the impact of the multipath on the GNSS measurements. To make these estimations it is important to find the data that can reflect the presence of multipath.
This data can come from various sources and have a more or less close relation to the actual presence of multipath. To give some examples, this data be related to the received signal, e.g. the received signal's power, to the position of the receiver with respect to the satellites, e.g. the elevation of the satellite, or the surroundings of the receiver, e.g. presence of buildings. 

The different measurements that are used to make the predictions are called features. In general, a larger quantity of different features will lead to a better performance of the estimations because there is more information about the variables that affect the outcome variable (i.e. the variable to estimate). Nonetheless, it is important that these features are related to the variable that we want to estimate, otherwise they will only add noise in the predicted value. For example, it does not make sense to use the stock market of a certain company when trying to predict the presence of multipath of a GNSS signal. Also, it is important that the features used are not directly related between them and that they add new information to the estimation. A counterexample of this would be to use both the Carrier-to-Noise-Density Ratio and the Signal-to-Noise Ratio, since these variables are directly related by the bandwidth of the receiver. Another important thing to consider is that a larger number of features requires a larger dataset of examples to perform the regression. Within this project, we also have an additional restriction in terms of selecting the features to use: these features should be accessible from a receiver at during navigation, so that the resulting model obtained from the regression could be applicable in a receiver to estimate the effect of multipath.

\section{Selection and analysis of features}
The execution of this project has been subject to the availability of data. At the very beginning we generated a random dataset which was used to implement the first regression method. Later on, we were provided of a dataset with few features, that was used to develop the algorithms and gain some knowledge of the data treatment. Nonetheless, this first dataset is of sufficient interest to include some of its results in this document. Afterwards, we obtained a second dataset which included more features and has been used to obtain the final concluding results. Both of these datasets come from measurements that were carried before the start of this project. These measurements were obtained with a van in various measurement campaigns in the city of Toulouse.

\subsection{Simple dataset}
We started the project with the outcome variable being the error in a pseudorange measurement, which had been estimated in post-processing. At this first stage we only had two features: the Carrier-to-Noise-Density Ratio (CN0) measured by the receiver and the estimated elevation of the satellite relative to the receiver. The purpose of this first stage was to develop and test the different regression algorithms, until final the data was available. Also, even though the pseudorange error includes other errors than the produced by the multipath, it is a value that is very related to the error we want to estimate in this project, so it was more interesting for us to familiarize with the characteristics of the data we were dealing with.





\subsection{Complete dataset}
The second and the main dataset that we use in this project is more complete. This dataset includes the multipath error as the outcome variable, instead of the whole pseudorange error that was used in the first dataset. This multipath error is estimated using an isolation method that tries to estimate all the other error sources in the pseudorange to obtain only the remaining multipath error \cite{matera_chatacterization_2019}. This second dataset also includes more features, which are described next.

\begin{itemize}
    \item \textbf{CN0:} Carrier-to-Noise-Density Ratio of the signal.
    \item \textbf{Elevation:} Elevation of the satellite relative to the receiver.
    \item \textbf{Relative azimuth:} Azimuth of the satellite relative to the vehicle's heading.
    \item \textbf{MTHL:} Maximum topographic height on the right side of the vehicle
\end{itemize}



% Introduction about the data we are using
%   Two stages in terms of data:
%       First we used Pseudo-range Error, CN0 and elevation to develop the algorithms and get familiar with (data?). Also, the other data was not available yet.
%       Later, we obtained more data and we predict the actual multipath error, which has been estimated **explain a little bit how and cite their paper**. We have more features -> we expect to have a better prediction

% Data preparation
%   remove outliers
%   remove invalid data (CN0 = 0, nan)
%   

% Data analysis:
%   show how y is distributed wrt to different features x. this may give some information about the relevance of those features in the estimation of y. 
%   explain plots: how x is discretized to show distribution and quantiles
%   comments about each plot: about expected relevance and so on
%         

%==================================================================================================================
\chapter{Fitting}\label{ch:fitting}

\section{MATLAB fitting}\label{matlabfit}

The reality we live is extremely complicated, therefore it is crucial to define models in order to understand it better. Regression analysis is a statistical tool which enables to establish an existing relationship between an output variable and one or multiple input variables. Therefore, it helps to depict the causal effect of certain parameters upon other ones \cite{IntroToRegresAna}. Regression models have been long used in many fields, such as economics, medicine or chemistry, principally for purposes related to predict, forecast and estimate values between observed data points \cite{MathWorksRegression}.  

Throughout this project, regression models have been considered in order to estimate the distribution of the errors present in the aforementioned features. This section focuses on nonlinear regression of the mean and standard deviation (STD) of the Multipath Error (PRE) as a function of elevation and Carrier-to-Noise (CN0) by using the Curve Fitting Toolbox\textsuperscript{\tiny\texttrademark} software developed for MATLAB\textsuperscript{\tiny\textregistered}. The aim of using this estimation technique is to provide a first intuition about the error modelling of the data initially provided. In addition, this fitting is a tool used to provide a reference of the resulting error model in order to compare the performance with other methods. Thus, the algorithms implemented can be validated as it was assumed that the results must be improved using more advanced techniques, such as Gradient Descent (GD) or Neural Networks (NN), which are able to generate the error model automatically only providing the dataset. 

The main idea of the regression method explained in this section is to define a cost function which characterizes the relation between two or more variables, so it can be used to predict the output cost of a certain input value even if it is not include in the given dataset. There exist many techniques to accomplish this goal, nonetheless, we will focus on the Least Squares (LS) method, which will be detailed further on. The core of this algorithm consists in minimizing the sum squared of the errors between the observed data and the prediction made by our model for the same points. By squaring the errors, the algorithm penalizes the worst points, those that present larger errors given that the predicted value is much further from the observed point. As a result, we obtain a combination of parameters which minimizes the cost function, so it represents the best relation (fit) between the input and output variables in order to obtain the lowest errors. 

%define a cost function which depicts the existing relationship between two or multiple variables and allows us to predict the value of the output variable even for input values that are not contained in the dataset 

The Curve Fitting Toolbox\textsuperscript{\tiny\texttrademark} includes a set of functions and utilities which allow to perform a regression by fitting a curve or surface to the data through linear and nonlinear models, or custom equations \cite{MathWorksRegression}. In consequence, fitting requires to define a parametric model which establishes a relation between a set of training data and the output variable using a number of coefficients, so that the output continuous-valued parameter can be predicted. 

Fitting a curve or a surface mainly depends on the number of features available. Thus, the measurement model is build in accordance with the number of features. In our case, there are two possible options: selecting one or two features. As a result, the algorithm fits a curve when one feature is selected (univariate nonlinear regression), otherwise it fits a surface (multivariate regression).

As we mentioned before, this software uses the Least Squares (LS) regression method to fit the data, which consists in obtaining the best estimator by minimizing the sum of squared residuals. For this fitting, two different nonlinear measurements models are considered: polynomial and exponential. 

Given a general nonlinear function $h$ to represent the measurement model for one feature, such that
\begin{equation}
    y = h(x,p) + \epsilon
\end{equation}
where $y$ is the output variable, $x$ is the input variable, $p$ corresponds to the coefficients of the model and $\epsilon$ corresponds to the error. $h$ may be also referred to as \textit{hypothesis}. Consequently, from the general model, it is possible to go into detail with respect to the aforementioned nonlinear models. 

On one side, a polynomial measurement model for a single feature is assumed, such that
\begin{equation}
    y_k = \sum\limits_{i=i}^{n+1} p_{i}x^{n+1-i},  \;\;\mbox{for}\; k = 1,...,M
\end{equation}

where $n+1$ is the order of the polynomial, which indicates the number of coefficient to be fit, and $n$ is the degree of the polynomial, which gives the highest power of the predictor variable. In addition, $x = [x_1,...,x_N]$ is the set of N know inputs, $y = [y_1,...,y_M]$ is the set of M observations and $p = [p_1,...,p_N]$ corresponds to the coefficient estimates.

The degree of the polynomial and, thus, the number of coefficients to be estimated may be modified according the problem requirements or needs. 
An example of the resulting model may be the following
\begin{equation}
\begin{split}
    y_k &= p_{1}x^{2} + p_{2}x + p_3,    \;\;\mbox{for}\; k = 1,...,M
\end{split}
\end{equation}

However, the toolbox allows to fit curves up to polynomials of degree 9, if desired. 

The main advantages of polynomial fits include reasonable flexibility for data that is not too complicated, and they are linear, which simplifies the fitting process \cite{MathWorksRegression}. However, high-degree models can become unstable and diverge extensively outside the data range \cite{MathWorksRegression}. 
% Explaining why polynomial here?

On the other side, it is also presented the one-term exponential measurement model, such that 
\begin{equation}
    y_k = p_{1}e^{p_{2}x_{k}},  \;\;\mbox{for}\; k = 1,...,M
\end{equation}
%% Explain why using exponential, advantages? disadvantages?

After introducing the measurements models implemented for one feature, the measurements models for two features are presented hereunder.

The nonlinear $h$ can be expressed as a function of two variables:
\begin{equation}
    y = h(x_{1},x_{2},p) + \epsilon
\end{equation}
where $y$ is the output variable, $x_{1}$ and $x_{2}$ are the features, $p$ corresponds to the coefficients of the model and $\epsilon$ corresponds to the error. 

This model can also be further detailed considering the polynomial model. It is important to notice that, in this case, the exponential model is not available for surface fitting in the MATLAB toolbox used. In consequence, the estimation of the mean and STD with two features has been carried out has been carried out using only a polynomial hypothesis.

An example of a polynomial measurement model with two features can be as follows:
\begin{equation}
\begin{split}
    y_k &= p_{00} + p_{10}x_{1} + p_{01}x_{2} + p_{11}x_{1}x_{2} + p_{20}x^{2}_{1} + p_{02}x^{2}_{2},    \;\;\mbox{for}\; k = 1,...,M
\end{split}
\end{equation}

In this case, the toolbox allows to fit a surface using polynomials up to degree 5. 

Once the measurement model is defined, it is time to introduce the main core of the fitting algorithm, which is the nonlinear Least Squares method. As it has been previously stated, the goal of the LS is to minimize the sum of squared residuals $r_k$ (SSR), so that the coefficient estimates $\hat{p}$ are obtained.
\begin{equation}\label{eq:SSR}
    SSR(p) = \sum\limits_{k=1}^{M}{r}_{k}^{2} = \sum\limits_{k=1}^{M}(y_k - \hat{y}_k)^2 = \sum\limits_{k=1}^{M}(y_k - \sum\limits_{i=i}^{n+1} p_{i}x^{n+1-i})^2
\end{equation}

Equivalently, equation \eqref{eq:SSR} can be expressed in a matrix way:
\begin{equation}
    SSR(p) = \sum\limits_{k=1}^{M}(Y - x_{i}p)^2 = (Y - xp)^T(Y - xp)
\end{equation}

Equation \eqref{eq:SSR} represents the cost function of the model, which gives a good measure of how well the particular model fits the dataset \cite{MachLearnRefined}. In order to obtain the minimum $\hat{p}$, this function must be minimized, which is accomplished by applying differentiation and scaling techniques:
\begin{equation}
\begin{split}
    \left.-\frac{1}{2}\pdv{(SSR(p))}{p_{j}}\right|_{\substack{\hat{p}}} &= x^{T}(Y - xp) = 0
\end{split}
\end{equation}

The final expression of the estimates is as follows:
\begin{equation}\label{eq:estimatorLS}
\begin{split}
    \hat{p} = (\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}Y = \textbf{S}Y\\
\end{split}
\end{equation}

where $\textbf{X}$ is the matrix of features, $\textbf{W}$ corresponds to the weight matrix and $\textbf{S}$ is called generalized inverse or pseudoinverse.

As nonlinear models are more difficult to fit than the linear ones, an iterative approach is required to perform instead of simple matrix techniques \cite{MathWorksRegression}. So, it is necessary to provide an initial estimate of the coefficients, which is set to [0,0] in this case. Hence, the function produces a initial fitted curve for the current set of coefficients. Then, the software adjusts the coefficients and determine whether the fit improves. Finally, it iterates the process until the fit reaches the specified convergence criteria \cite{MathWorksRegression}. 

In addition to the nonlinear LS, we have also implemented a nonlinear Weighted Least Squares (WLS). This variant of the LS is specially useful when the assumption such that the output data has constant variance is no longer fulfilled. Therefore, the WLS is a powerful tool to improve the fit, which consists in including additional weights in the fitting process in order to give more importance (more weight) to the high-quality data, which will have a greater impact on the final coefficient estimates. 

Due to the weights, defined as $w_{k}$, the expression of the SSR in equation \eqref{eq:SSR} results in
\begin{equation}\label{eq:SSR_2}
    SSR(p) = \sum\limits_{k=1}^{M}w_{k}{r}_{k}^{2} = \sum\limits_{k=1}^{M}w_{k}(y_k - \hat{y}_k)^2
\end{equation}

Accordingly, 
\begin{equation}
    SSR(p) = (Y - xp)^T\textbf{W}(Y - xp)
\end{equation} 

where $\textbf{W} = \frac{1}{h(x,p)}\textbf{I}$, considering $\textbf{I}$ an identity matrix. 

So, the formulation of the estimates is modified such that
\begin{equation}\label{eq:estimatorWLS}
\begin{split}
    \hat{p} = (\textbf{X}^{T}\textbf{W}\textbf{X})^{-1}\textbf{X}^{T}\textbf{W}Y\\
\end{split}
\end{equation}

\subsection{Architecture of our curve fitting}

Once the fundamentals of the curve fitting are understood, we can proceed to introduce the structure of the algorithm implemented in order to clarify the steps carried out for the present regression method.

First of all, as it has been mentioned previously, the fitting is conducted in order to obtain the coefficient estimates of the mean ($\mu$) and STD ($\sigma$) of the PRE. To achieve this objective, it is important to differentiate both types fittings. 

Regarding to the mean fitting, the input parameters of the function correspond to the given features and the observed data points. In this case, the features are the elevation and CN0 and the observed data corresponds to the PRE. The magnitude of the PRE depends on many factors and, among them, the aforementioned ones are present. Hence, both inputs are necessary to perform the LS regression and, therefore, obtaining the output, which corresponds to the estimate of the variable being measured: the mean of the PRE.  

As a quick reminder of the previous chapter \ref{ch:data_analysis}, at this point, the data has already been processed in order to discard invalid or undesired values. 

In this sense, the procedure followed for the computation of the mean fitting can be resumed with the following steps:
\begin{enumerate}
    \item Define fitting options,
    \item Introduce features and observed data points,  
    \item Apply curve fitting function to estimate the coefficients of the model,
    \item Evaluate the regression model with the input data and the coefficients estimates,
    \item Compute Root Mean Squared Error (RMSE). 
\end{enumerate}

Regarding the RMSE, it represents a measure of the error made by the model in predicting the observed data. The RMSE of predicted values is computed as follows:

\begin{equation}
    RMSE = \sqrt{\frac{\sum\limits_{k=1}^{M}{r}_{k}^{2}}{M}} = \sqrt{\frac{\sum\limits_{k=1}^{M}{r}_{k}^{2}}{M}} = \sqrt{\frac{\sum\limits_{k=1}^{M}(y_k - \hat{y}_k)^2}{M}} 
\end{equation}

This computation is equivalent for the mean and the STD estimation, as well as for the WLS. The only consideration is that the observed values $y$ and predicted ones $\hat{y}$ represent different parameters in each case.  
 
Hence, RMSE is a useful tool to get a measure of the spread of the $y$ values about the predicted $\hat{y}$ values \cite{StanfordRMSE}. As the effect of each error on the RMSE is proportional to the size of the squared error, then larger errors have a disproportional impact on the RMSE, leading to extremely large errors. As a result, RMSE is very sensitive to outliers. In this case, the RMSE is a parameter used to analyze and compare the performance of the different regression models implemented in this project.   

On the other hand, the fitting of the STD carries out the estimation of the error of the observed variables, in this case the PRE, with respect to the features provided. This particularity requires a previous step in comparison with the mean fitting. As the regression is applied to the error of the observed parameter instead of the total value, then the residuals are additionally computed prior to perform the fitting.  

So, the input variable or vector corresponds to:
\begin{equation}\label{eq:inputstd}
\begin{split}
    r_{k,\sigma} = |y_k - \hat{y}_{k,\mu}| = |y_k - h(x_{k},\hat{p}_{k,\mu})| = |y_k - x_{k}p_{k,\mu}|\\
\end{split}
\end{equation}

Once the computation is made according to equation \eqref{eq:inputstd}, the same steps as previously stated for the mean fitting are followed. 

Finally, this procedure is repeated for the nonlinear WLS regression method, in which the computation of the weights must be taken into consideration before the fitting for both mean and STD. 

% The main reason behind the decision to select a polynomial model is the distribution of the data provided. According to the results presented in chapter \ref{ch:data_analysis}, 

\section{Gradient descent}

Gradient descent is an optimization algorithm which has become very popular in the last years as it is a crucial algorithm for training machine learning models. It is present in most Artificial Intelligence (AI) systems which are being developed these days. However, its origin dates back to the 19th century, when it was developed by Lous Augustin Cauchy in his \textit{Compte Rendu à l’Académie des Sciences} of October 18, 1847 \cite{cauchy}.

First of all, it is important to recall the main idea behind the regression technique detailed in section \ref{matlabfit}. The objective was to minimize a given cost function, also known as hypothesis or simply $h$, in order to find the parameters such that the value of the cost function evaluated at those parameters corresponds to the global minima of the function. As the cost function measures the accuracy of the hypothesis of our model to predict the output variable, then finding the minimum of $h$ implicates that the error made by the prediction model is the lowest. Hence, it is very interesting to develop advanced techniques in order to optimize this minimization problem. 

Before expatiating upon GD, it is necessary to introduce some mathematical concepts related to functions. In particular, some important properties of a convex function and their implications on GD. 

The main characteristic of a convex function is that there exists a stationary point at which the function's derivative is zero, assuming that this function is differentiable. In a very informal way, at this point, the slope of function is zero, so it neither increases or decreases. In addition, there is another property which states that, in case of finding a minimium point in the function, it is in fact a global minimum. That is to say that there are no other points located below that this one on the function. 

However, the algorithm may encounter a type of functions, also known as non-convex, which presents other minimums (local minima) besides the global minima. So the algorithm may have to deal with multiple minimum points and, therefore, multiple equations to solve, which correspond to a derivative equal to zero for each point. Besides the local minima, there also other points which can fulfill the same premise, so the system of equations can become extremely large. This can result in a serious problem for optimization algorithms, not only in terms of computational efficiency, but also because they can get stuck in a local minima and may never find the global one. In consequence, it is essential to find a solution for the non-convex functions. 

The GD method aims to solve this problem by presenting an intuitive reasoning to find the minimum point of the function. 
% la logica destras del gd es tantear la inclinacion en el punto que nos encontramos luego nos movemos hacia el punto en el que la pendiente desciende com mayor intensidad para finalmente encontrar el punto minimo. 
%por tanto primero evaluaamos la inclinacion para localizar la mayor pendiente en la posicion actual, luego avanzamos en esa direccion, luego nos paramos en una nueva posicion y repetimos el proceso  iterativamente hasta convergencia, 
%lo que vamos a hacer es aprovechar la informacion que nos proporciona el calculo de la derivada para ayudarnos a encontrar los minimos locales, ya que la derivada nos informa de la pendiente de la funcio. 








%the inverse requires a large computational cost, so gd is better






\section{Neural Network}
% Introduction
The concept of Artificial Neural Networks, also known just as neural networks, was first introduced in the early 1940s by McCullock and Pitts, who theorized about a computational model for artificial intelligence which was inspired on the neural networks in the brain \cite{mcculloch_logical_1943}. Later, in 1958, Rosenblatt developed the perceptron which can be considered as the equivalent of a neuron in Artificial Neural Networks \cite{rosenblatt_perceptron_1958}. The perceptron allowed to learn from a set of example data and later perform binary classification of new examples. In 1969 it was demonstrated that perceptrons were incapable of performing non-linear classifications such as performing the logical XOR operation \cite{minsky69perceptrons}. The development of the backpropagation algorithm in 1975 renewed the interest in neural networks because it allowed to train multiple-layer perceptrons \cite{werbos}. These were networks of various neurons that allowed the learning of non-linear functions. Even then, neural networks were rarely used due to the high computational complexity and the large amounts of data that they required. An increase in transistor count and a latter raise in the use of GPUs have bring back to life the neural networks, also the Internet explosion allowed a much easier collection of large amounts of data, which are needed to train neural networks \cite{aggarwal_neural_2018}.

Lately, neural networks have outperformed other regression algorithms in many complex problems and have also empowered new and unforeseen applications such as vehicle control \cite{BojarskiTDFFGJM16}, image recognition \cite{hijazi2015} or medical diagnosis \cite{AMATO201347}.

\subsection{Model representation}
The basic element in a neural network is called neuron. A neuron is nothing more that an element that takes some weighted inputs, adds them to obtain $z$ and maps the result to another value, $g(z)$, which is the output. The function $g(z)$ is called \textit{activation function} and it is the main characteristic of the neuron. The behavior of a neuron is expressed as

\begin{equation}
    g(z) = g(\btheta^T\boldsymbol{x})
\end{equation}

where $\x = [x_0, x_1, x_2, ..., x_n]^T$ are the input values $x_0 = 1$, and $\btheta = [\theta_0, \theta_1, \theta_2, ..., \theta_n]$ are the weights assigned to these input values. The idea of this model is that, given the correct weights to the input values and the right activation function, it will estimate a value $y^{(i)}$ which depends on the input $\boldsymbol{x^{(i)}}$ as $h_\theta(\boldsymbol{x^{(i)}})$. This neuron can be represented with the diagram in Fig. \ref{fig:neuron_diag}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fitting/neuron_diagram.png}
    \caption{Diagram representation of a neuron}
    \label{fig:neuron_diag}
\end{figure}

In order to find the best weights $\btheta$, the error function is defined as we did in the previous section and it can be minimized with the gradient descent method. As it has been previously mentioned, a single neuron does not work well for complex non-linear processes. To be able to make predictions in those cases, a more complex system is required. 

Neural networks are built by using the outputs of various neurons as the inputs for other neurons, then building layers. The first one is called \textit{input layer}, the last one, \textit{output layer} and the layers in between are usually called \textit{hidden layers}. Usually, a neural network with a high number of layers will be able to perform more complex predictions but it will also take more time to train.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fitting/neural_network_example.png}
    \caption{Example of a neural network}
    \label{fig:neural_network_example}
\end{figure}

An example of a neural network is shown in Fig. \ref{fig:neural_network_example}. In this diagram, $a_i^{(j)}$ denotes the result of evaluating the activation function at the unit $i$ of the layer $j$. It is important to remark that $x_0$ and the activation values $a_0^{(j)}$, which are known as bias, are always set to 1. This is the reason why these nodes do not have any input. Also, we define $\bTheta^{(j)}$ as the matrix of weights controlling the function mapping from layer $j$ to $(j+1)$. The procedure to estimate $h_\Theta(\x)$, also known as forward propagation, consists on computing the activation $a_i^{(j)}$ at each one of the units in the neural network, starting from the input layer and moving forward to the output layer. This process is shown below for the example in the diagram of Fig. \ref{fig:neural_network_example}.

\begin{enumerate}
    \item Activation of layer 2
    \begin{equation}
    \begin{split}
        a_1^{(2)} &= g(z_1^{(2)}) = g(\bTheta_{10}^{(1)} + \bTheta_{11}^{(1)} x_1 + \bTheta_{12}^{(1)} x_2) \\
        a_2^{(2)} &= g(z_2^{(2)}) = g(\bTheta_{20}^{(1)} + \bTheta_{21}^{(1)} x_1 + \bTheta_{22}^{(1)} x_2) \\
        a_3^{(2)} &= g(z_3^{(2)}) = g(\bTheta_{30}^{(1)} + \bTheta_{31}^{(1)} x_1 + \bTheta_{32}^{(1)} x_2)
    \end{split}
    \end{equation}
    \item Activation of layer 3
    \begin{equation}
    \begin{split}
        a_1^{(3)} &= g(z_1^{(3)}) = g(\bTheta_{10}^{(2)} + \bTheta_{11}^{(2)} a_1^{(2)} + \bTheta_{12}^{(2)} a_2^{(2)} + \bTheta_{13}^{(2)} a_3^{(2)}) \\
        a_2^{(3)} &= g(z_2^{(3)}) = g(\bTheta_{20}^{(2)} + \bTheta_{21}^{(2)} a_1^{(2)} + \bTheta_{22}^{(2)} a_2^{(2)} + \bTheta_{23}^{(2)} a_3^{(2)}) \\
        a_3^{(3)} &= g(z_3^{(3)}) = g(\bTheta_{30}^{(2)} + \bTheta_{31}^{(2)} a_1^{(2)} + \bTheta_{32}^{(2)} a_2^{(2)} + \bTheta_{33}^{(2)} a_3^{(2)})
    \end{split}
    \end{equation}
    \item Activation of layer 3: prediction
    \begin{equation}\label{eq:nn_cost_function}
        h_\Theta(\x) = a_1^{(3)} = g(z_1^{(4)}) = g(\bTheta_{10}^{(3)} + \bTheta_{11}^{(3)} a_1^{(3)} + \bTheta_{12}^{(3)} a_2^{(3)} + \bTheta_{13}^{(3)} a_3^{(3)})
    \end{equation}
\end{enumerate}

The architecture of a neural network is mainly defined by the number of layers, the number of units in each layer and the activation functions that are used. The number of units of the input and output layers is usually restricted by the characteristics and goals of the application at hand. The number of input nodes is determined by the number of features that are used, since this layer is indeed used to feed the network with data. The number of output units can be larger than one and it is chosen depending on the type of problem that is confronted. For example, a classification problem requires as many output units as classes, so that the output of each unit $k$ represents the probability of the input example to pertain to the class $k$. 

The size of the hidden layers, both in terms of number of layers and number of units per layer, is generally a difficult part in the design of a neural network since there is no direct rule to decide among the different possibilities. Therefore, these are usually chosen by trial and error procedures. On the one hand, having too small hidden layers may lead to underfitting or even the inability of the algorithm to converge. On the other hand, having too large and/or too many hidden layers will increase the computation complexity and may also lead to overfitting \cite{aggarwal_neural_2018}. Nonetheless, a big majority of problems can be faced using a network with one hidden layer \cite{aggarwal_neural_2018}. Actually, it has been shown that a single hidden layer of nonlienar units and a single linear output layer can compute almost any "reasonable" function \cite{hornik_multilayer_1989}. Although this theoretical claim is not always easy to apply because it may require a huge amount of features.

The choice of the activation cost function is a critical decision in the design of a neural network. Any network with all the activation functions being linear will provide the same results as a Least Squares. In order to obtain more complex behavior, some unlinearities need to be added. To do so, many options of activation functions are proposed. The performanece of these different activation functions also depends on the type of problem that is being faced, but it usually requires some trial and error as well. Next, some examples of activation functions are shown.

\begin{itemize}
    \item Linear function
    \begin{equation}\label{eq:act_linear}
        g(z) = z
    \end{equation}
    \item Sigmoid function
    \begin{equation}\label{eq:act_sigmoid}
        g(z) = \frac{1}{1+e^{-z}}
    \end{equation}
    \item Tanh function
    \begin{equation}\label{eq:act_tanh}
        g(z) = \frac{e^{2z} - 1}{e^{2z} + 1}
    \end{equation}
    \item Rectifier Linear Unit (ReLU)
    \begin{equation}\label{eq:act_relu}
        g(z) = \max(0, z)
    \end{equation}
\end{itemize}



\subsection{Training a neural network}
Training a neural network basically consists in finding the weights for the $L$ layers of that network, $\bTheta = \{\bTheta^{(1)}, \bTheta^{(2)}, ..., \bTheta^{(L)}\}$, that minimize the cost function of the neural network, which describes the error between the actual measurements and the predictions. This is

\begin{equation}
    \min_{\bTheta} J(\bTheta) = \min_{\bTheta} \frac{1}{2m} \left(\sum_{i=1}^{m} \left(y^{(i)} - h_\Theta (\x^{(i)}) \right)^2 \right) + \frac{\lambda}{2m} \sum_{l=1}^{L} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\bTheta_{j,i}^{(l)})^2
\end{equation}

where $m$ is the number of examples in the data set, $\lambda$ denotes the regularization term, and $s_l$ is the number of units of the layer $l$.

To obtain this minimum in $J(\bTheta)$ we use the gradient descent. The problem that we encounter is that, given the very high complexity of this cost function, we virtually cannot compute its gradient. This is where the backpropagation algorithm appears. This method allows to obtain the equivalent of the gradient for a neural network in a much more efficient way.

This algorithm starts with a random initialization of the parameters $\bTheta$. Then, it performs forward propagation with an example $(\x^{(i)}, y^{(i)})$ to obtain an estimation $h_\Theta(\x^{(i)})$. Once the estimation is obtained, the error of $h_\Theta(\x^{(i)})$ with respect to $y^{(i)}$ is obtained. This error is then "propagated" back in the network, so that we obtain a representation of the contribution of each node to the final error. This process is then repeated for each one of the examples given in the training set. The exact procedure of this algorithm is detailed next.

Given a training set $\set{(\x^{(1)}, y^{(1)}), (\x^{(2)}, y^{(2)}), ... (\x^{(m)}, y^{(m)})}$

\begin{enumerate}
    \item Set $\bDelta_{ij}^{(l)} = 0$ , $\forall l, i, j$
    \item Randomly initialize $\bTheta = \{\bTheta^{(1)}, \bTheta^{(2)}, ..., \bTheta^{(L)}\}$
    \item For $i$ from 1 to m:
    \begin{enumerate}[i.]
        \item Perform forward propagation to obtain $\ba ^{(l)}$ for $l=2, 3, ..., L$
        \item Calculate $\delta^{(L)} = a^{(L)} - y$
        \item For l from $(L-1)$ to 2:
        \begin{enumerate}[a.]
            \item $\bdelta^{(l)} = (\bTheta^{(l)})^T\bdelta^{(l+1)} \odot g'(\bz^{(l)})$\footnote{The symbol $\odot$ denotes the Hadamard or element-wise product}
        \end{enumerate}
        \item Update $\bDelta^{(l)}$ as $\bDelta^{(l)} := \bDelta^{(l)} + \bdelta^{(l+1)}(\ba^{(l)})^T$
    \end{enumerate}
    \item End for
    \item Calculate $\bD_{ij}^{(l)} = \frac{1}{m} \bDelta_{ij}^{(l)}$ if $j = 0$
    \item Calculate $\bD_{ij}^{(l)} = \frac{1}{m} \bDelta_{ij}^{(l)} + \lambda \bTheta_{ij}^{(l)}$ if $j \neq 0$
\end{enumerate}

The final result that we obtain after the backpropagation algorithm is equivalent to the gradient of the cost function of the neural network

\begin{equation}
    \bD_{ij}^{(l)} = \pdv{}{\bTheta_{ij}^{(l)}} J(\bTheta)
\end{equation}

% Overfit / Underfit (here?)

\subsection{Architecture of our neural network}
Given the nature of the neural networks, there exist an infinity of possibilities when trying to decide the architecture of a network. Nonetheless, it is usually a good practice to start with a simple design and then, if needed, move to more complex configurations. In our case we started with one hidden layer which has the same number of units as the input layer. This design is widely recommended as it has been found to perform well in a large variety of cases. Also, since we are dealing with a linear regression problem we need to have only one unit in the output layer.

The other defining characteristic of a neural network is the activation function that is used. On the one hand, we decided to use a ReLU activation function for the units in the hidden layer. This activation function has shown very good performance in linear regression problems. On the other hand, the output layer implements a linear activation function, because the possible output values that we can have for our problem are not bounded.

%==================================================================================================================

\chapter{Results} \label{ch:results}

%=================================================================================================================================================

\chapter{Conclusions}\label{ch:conclusions}

\clearpage
\printbibliography

\end{document}