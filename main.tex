\documentclass[a4paper, report, oneside, UKenglish]{memoir}


%% Title page
\usepackage{projectfp}


%% Silence warning about obsolete package
\usepackage{silence}
\WarningFilter{remreset}{The remreset package}


%% Encoding
\usepackage[utf8]{inputenx} % Source code
\usepackage[T1]{fontenc}    % PDF


%% Fonts and typography
\usepackage{lmodern}           % Latin Modern Roman
\usepackage[scaled]{beramono}  % Bera Mono (Bitstream Vera Sans Mono)
\renewcommand{\sfdefault}{lmss} % Latin Modern Sans Serif
\usepackage[final]{microtype}  % Improved typography
\renewcommand{\abstractnamefont}{\Large\sffamily\bfseries}           % Abstract
\renewcommand*{\chaptitlefont}{\Huge\bfseries\rmfamily\raggedright}  % Chapter
\setsecheadstyle{\Large\bfseries\sffamily\raggedright}               % Section
\setsubsecheadstyle{\large\bfseries\sffamily\raggedright}            % Subsection
\setsubsubsecheadstyle{\normalsize\bfseries\sffamily\raggedright}    % Subsubsection
\setparaheadstyle{\normalsize\bfseries\sffamily\raggedright}         % Paragraph
\setsubparaheadstyle{\normalsize\bfseries\sffamily\raggedright}      % Subparagraph
\setsecnumdepth{subsection}

\usepackage{geometry}

%% Mathematics
\usepackage{amsmath}
\usepackage{amssymb}   % Extra symbols
\usepackage{amsthm}    % Theorem-like environments
\usepackage{thmtools}  % Theorem-like environments
\usepackage{mathtools} % Fonts and environments for mathematical formulae
\usepackage{mathrsfs}  % Script font with \mathscr{}
\usepackage{physics}

%% Miscellanous
\usepackage{graphicx}  % Tool for images
\graphicspath{{figures/}}
\usepackage{babel}     % Automatic translations
\usepackage{csquotes}  % Quotes
\usepackage{textcomp}  % Extra symbols
\usepackage{listings}  % Typesetting code
\lstset{basicstyle = \ttfamily, frame = tb}
\usepackage{multirow}  % Multirow equations
\usepackage[table,xcdraw]{xcolor} % Color for tables
\usepackage{gensymb}   % Symbols
\usepackage[center]{caption}
\usepackage{parskip}
\usepackage[hidelinks]{hyperref} % Hide red boxes around links


%% Bibliography
\usepackage[style=ieee]{biblatex}
\usepackage{mathscinet}
% \usepackage[backend    = biber,
%             sortcites  = true,
%             giveninits = true,
%             doi        = false,
%             isbn       = false,
%             url        = false,
%             style      = alphabetic
%             citestyle  = numeric]{biblatex}
% \DeclareNameAlias{sortname}{family-given}
% \DeclareNameAlias{default}{family-given}
% \DeclareFieldFormat[article]{volume}{\bibstring{jourvol}\addnbspace#1}
% \DeclareFieldFormat[article]{number}{\bibstring{number}\addnbspace#1}
% \renewbibmacro*{volume+number+eid}
% {
%     \printfield{volume}
%     \setunit{\addcomma\space}
%     \printfield{number}
%     \setunit{\addcomma\space}
%     \printfield{eid}
% }
\bibliography{bibliography}
\addbibresource{bibliography.bib}

%% Cross references
%\usepackage{varioref}
%\usepackage[pdfusetitle]{hyperref}
%\urlstyle{sf}
%\usepackage[nameinlink, capitalize, noabbrev]{cleveref}
%\crefname{chapter}{Section}{Sections}

%% Delimiters
\DeclarePairedDelimiter{\paren}{\lparen}{\rparen}   % Parenthesis
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace} % Set
% \DeclarePairedDelimiter{\abs}{\lvert}{\rvert}   % Absolute value
% \DeclarePairedDelimiter{\norm}{\lVert}{\rVert}  % Norm


%% Operators
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\DeclareMathOperator{\im}{im}
% \DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

%% New commands for vectors
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bD}{\boldsymbol{D}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bTheta}{\boldsymbol{\Theta}}


%% New commands for sets
\newcommand{\N}{\mathbb{N}}   % Natural numbers
\newcommand{\Z}{\mathbb{Z}}   % Integers
\newcommand{\Q}{\mathbb{Q}}   % Rational numbers
\newcommand{\R}{\mathbb{R}}   % Real numbers
\newcommand{\C}{\mathbb{C}}   % Complex numbers
\newcommand{\A}{\mathbb{A}}   % Affine space
\renewcommand{\P}{\mathbb{P}} % Projective space


%% Miscellanous
\renewcommand{\qedsymbol}{\(\blacksquare\)}
\setlength{\parindent}{0pt} % Indent before each paragraph
\setlength{\parskip}{7pt}   % Space between paragraphs


\title{ML Project}
\author{Andrea Bellés Ferreres \newline Arnau Ochoa Bañuelos}
\supervisor{Carl Milner\newline Heekwon No}

\begin{document}

\projectfrontpage

\newpage
\begin{abstract}
\noindent

\end{abstract}

\newpage
\begin{KeepFromToc}
    \tableofcontents
\end{KeepFromToc}

\newpage
\chapter{Introduction}\label{ch:intro}
Global Navigation Satellite System (GNSS) refers to a constellation of satellites transmitting signals from space that provide positioning and timing data to GNSS receivers. Then, these receivers use the data to determine the their own location.

Satellite-based systems present many advantages with respect to other positioning systems, such that global coverage and long-term accuracy with errors limited to a few meters \cite{groves}, considering a stand-alone standard GNSS receiver. Consequently, this type of navigation systems are being employed in a wide range of applications, not only for positioning purposes, but also to improve traffic flow, assist in civil protection and rescue operations, and research on subjects such that geodesy, meteorology and surveying, among many others \cite{EGSAwebsite}. 

The rising demand of real-time information entiles the use of GNSS in many fields, such as the European railway network. In December 2019, the European Commission invested in the development of a new research project, known as Certifiable Localisation Unit with GNSS  (CLUG) in the railway environment, within the framework of the Horizon 2020 programme. This EU project aims to assess the feasibility of a failsafe on-board multi-sensor localisation unit using GNSS, track maps and a minimal number of reference points \cite{CORDISwebsite}. It will also review the use of an on-board continuous localisation system that provides information on location, speed and other dynamics of the train \cite{CORDISwebsite}. The future purpose is the development of train digitalisation and automation. 

In addition to the coordinate collaboration of multiple railway companies, railway signalling industries and navigation specialists \cite{CORDISwebsite}, part of this project is carried out in the École Nationale de l'Aviation Civile (ENAC). It is proposing to develop an innovate model of the local errors impacting the GNSS receiver. Besides that, ENAC's role consists in helping to develop the end-to-end integrity concept including an assesment of the error and fault models.  

In order to support this work, ENAC is collecting its own data within the complex urban environments that the CLUG project is addressing by using the "Jumpy" research vehicle of the institution.

As part of the work developed by ENAC, this project aims to contribute to the development of different regression techniques to generate a model for the local errors. Concretely, the objective of this project is to develop a regression algorithm capable of estimating the effect of the multipath error from data that can be obtained in real-time. The main goal is to test if this regression can be performed with a Neural Network and how its performance compares to other regression algorithms, namely the fitting tool provided by Matlab\footnote{https://es.mathworks.com/help/curvefit/fit.html} and the Gradient Descent method. Each of them will be further detailed in the following chapters. 

Since Neural Networks techniques are relatively recent, not many applications have been done in the field of GNSS and even less have been proposed for multipath detection. Munin et al. propose the use of convolutional neural networks to estimate the presence of multipath error from an image representation of the raw data obtained from the correlator in \cite{munin_convolutional_2019}. Quan et al. follow another approach in \cite{quan_convolutional_2018}, where they perform classification of the signals in terms of multipath presence applying techniques of feature extraction. In our case, we are using a feedforward neural network to estimate the magnitude of the multipath error in a measured pseudorange from various indicators such as the signal power or the satellite elevation relative to the receiver.

In this document we will start in chapter \ref{ch:data_analysis} by analyzing the data that will be later used. There we will also explain how we prepared the data so that it can help the performance of the algorithms and the analysis of the results. Later, in chapter \ref{ch:fitting} we detail the algorithms that we have implemented to perform the regression. The results are shown and analyzed in chapter \ref{ch:results}. Finally, the conclusions are drawn in chapter \ref{ch:conclusions}.

%%% todo: resume of what are we going to do mentioning where it is going to be explain:
%%%% Ex: Throughout this document we will analyse.... blablabla in chapter 1.... sth like this
%%Throughout this project, regression models have been considered in order to solve a predictive problem with regard to the modelling of local errors present in a set of features, which have been provided by the supervisors of this project, and the corresponding polynomial relationship of those features which best captures the error distribution

% s'han desenvolupat els metodes fent servir MATLAB fitting, pero despres el analisis principal es amb mes features. Mes que res perque 
% 1. sabem (o intuim bastant) que modelar el mp amb cno i elev es dificil, pq no ens donen prou info
% 2. la gracia de fer servir NN es poder analitzar mes features de les que analitzaries amb altres metodes (i trobar patrons mes complexos), del que ho faries amb un LS o GD. (a part del aprenentatge automatic i tal) 
% crec que aquest es com el main purpose del treball, rollo la hipotesi que tenim i tal

        
% ...

% Fitting (o regression o whatever)
%     matlab fitting -> first reference
%     gradient descent -> interesting method (?), reference for NN
%     neural networks -> expected to have better performance since it allows to find more complex relations between data (aprox)

%==================================================================================================================
\chapter{Data analysis}\label{ch:data_analysis}
%**** explain the concept of features

% Introduction
As it has been previously explained, the purpose of this project is to develop a regression method capable of predicting the impact of the multipath on the GNSS measurements. To make these estimations it is important to find the data that can reflect the presence of multipath.
This data can come from various sources and have a more or less close relation to the actual presence of multipath. To give some examples, this data be related to the received signal, e.g. the received signal's power, to the position of the receiver with respect to the satellites, e.g. the elevation of the satellite, or the surroundings of the receiver, e.g. presence of buildings. 

The different measurements that are used to make the predictions are called features. In general, a larger quantity of different features will lead to a better performance of the estimations because there is more information about the variables that affect the outcome variable (i.e. the variable to estimate). Nonetheless, it is important that these features are related to the variable that we want to estimate, otherwise they will only add noise in the predicted value. For example, it does not make sense to use the stock market of a certain company when trying to predict the presence of multipath of a GNSS signal. Also, it is important that the features used are not directly related between them and that they add new information to the estimation. A counterexample of this would be to use both the Carrier-to-Noise-Density Ratio and the Signal-to-Noise Ratio, since these variables are directly related by the bandwidth of the receiver. Another important thing to consider is that a larger number of features requires a larger dataset of examples to perform the regression. Within this project, we also have an additional restriction in terms of selecting the features to use: these features should be accessible from a receiver at during navigation, so that the resulting model obtained from the regression could be applicable in a receiver to estimate the effect of multipath.

\section{Selection and analysis of features}
The execution of this project has been subject to the availability of data. At the very beginning we generated a random dataset which was used to implement the first regression method. Later on, we were provided of a dataset with few features, that was used to develop the algorithms and gain some knowledge of the data treatment. Nonetheless, this first dataset is of sufficient interest to include some of its results in this document. Afterwards, we obtained a second dataset which included more features and has been used to obtain the final concluding results. Both of these datasets come from measurements that were carried before the start of this project. These measurements were obtained with a van in various measurement campaigns in the city of Toulouse. This van was equipped with a wide-lens camera facing up, which was used to obtain data of the surroundings.

\subsection{Simple dataset}
We started the project with the outcome variable being the error in a pseudorange measurement, which had been estimated in post-processing. At this first stage we only had two features: the Carrier-to-Noise-Density Ratio (CN0) measured by the receiver and the estimated elevation of the satellite relative to the receiver. The purpose of this first stage was to develop and test the different regression algorithms, until final the data was available. Also, even though the pseudorange error includes other errors than the produced by the multipath, it is a value that is very related to the error we want to estimate in this project, so it was more interesting for us to familiarize with the characteristics of the data we were dealing with. This dataset contains more than 8.2 million useful examples.

Figures \ref{fig:pre_cn0} and \ref{fig:pre_elev} show the distribution of the pseudorange error as a function of the CN0 and Elevation, respectively. The graphs on the top show the actual data examples represented as points. Given the large amount of measurements, which makes more difficult to distinguish the actual distribution, we also show the 0.5 and 0.95 quantiles of the data. This quantiles are calculated at intervals of one unit of the independent variable, or feature. The graphs on the bottom of the figures represent the amount of examples for each value of the CN0 or elevation, respectively.

In these figures we can observe that both these features are representative for trying to estimate the pseudorange error. In both cases, the PR error decreases when the value of the features increases. This is expected, since the CN0 and the elevation of the satellite are very related to the quality of the signal. Nonetheless, we can observe that the examples in the dataset are mostly located in medium and high values of the CN0. This may lead to a low performance of the estimation algorithm for cases when the CN0 is low, since the data we have for those cases is less representative.

\begin{figure}[hb]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/pre_cn0.png}
    \caption{Distribution of the pseudorange error as a function of the CN0}
    \label{fig:pre_cn0}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/pre_elev.png}
    \caption{Distribution of the pseudorange error as a function of the elevation}
    \label{fig:pre_elev}
  \end{minipage}
\end{figure}

These two features seem to be good indicators of the presence of large errors in the pseudorange measurement. Nonetheless, we know that the pseudorange measurement is affected by many other error sources. Therefore we do not expect to estimate with a good accuracy the pseudorange errors having only these two measurements. Nonetheless, we believe that it will be interesting to analyze the performance of the different algorithms that are studied within this project.



\subsection{Complete dataset}
The second and the main dataset that we use in this project is more complete. This dataset includes the multipath error as the outcome variable, instead of the whole pseudorange error that was used in the first dataset. This multipath error is estimated using an isolation method that tries to estimate all the other error sources in the pseudorange to obtain only the remaining multipath error \cite{matera_chatacterization_2019}. This second dataset contains more than 78000 examples and it also includes more features, which are presented next:

\begin{itemize}
    \item \textbf{CN0:} Carrier-to-Noise-Density Ratio of the signal.
    \item \textbf{Elevation:} Elevation of the satellite relative to the receiver.
    \item \textbf{Relative azimuth:} Azimuth of the satellite relative to the vehicle's heading.
    \item \textbf{MTHL:} Maximum topographic height on the left side of the vehicle.
    \item \textbf{MTHR:} Maximum topographic height on the right side of the vehicle.
\end{itemize}

\begin{figure}[htb]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/mpn_cn0.png}
    \caption{Distribution of the multipath error as a function of the CN0}
    \label{fig:mpn_cn0}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/mpn_elev.png}
    \caption{Distribution of the multipath error as a function of the elevation}
    \label{fig:mpn_elev}
  \end{minipage}
\end{figure}

\begin{figure}[htb]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/mpn_maxtopol.png}
    \caption{Distribution of the multipath error as a function of the MTHL}
    \label{fig:mpn_mthl}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{data-analysis/mpn_maxtopor.png}
    \caption{Distribution of the multipath error as a function of the MTHR}
    \label{fig:mpn_mthr}
  \end{minipage}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{data-analysis/mpn_daz.png}
    \caption{Distribution of the multipath error as a function of the relative azimuth}
    \label{fig:mpn_daz}
\end{figure}

Figures \ref{fig:mpn_cn0} to \ref{fig:mpn_daz} represent the distribution of multipath error with respect to the different features in this dataset. As we observed in the previous dataset, the CN0 and elevation features are also representatives of the multipath error. Additionally, the new features regarding the topography data also show a correlation with the multipath error (see Figures  \ref{fig:mpn_mthl} and \ref{fig:mpn_mthr}). This is the expected because an scenario with taller buildings leads to higher multipath errors due to a larger amount of reflections. Finally, the differential azimuth also seems to be correlated with the multipath error. Figure \ref{fig:mpn_daz} shows that the multipath error is larger when satellites are located at both sides of the vehicle, i.e. around 90 and 270 degrees. This is also an expected behavior because the larger obstacles will be usually located at the sides of the vehicle instead of at the front or back, where there is the road.

After this analysis, we expect that the addition of this features into the algorithm will improve the accuracy in the estimation of the multipath error, with respect to other simpler methods.



\section{Data preparation}\label{DataPrep}
Before applying a regression algorithm is usually recommended to do some data preparation. First, it is important to check for invalid data and remove it. For both datasets used in this project, we have cleaned the data of examples that had NaN values and other examples that had other invalid values, such as a CN0 of 0 dB-Hz. Also, we have removed the outliers by setting two threshold of [-100, 300] meters as minimum and maximum values for the pseudorange and multipath errors.

Once the datasets are clean of invalid values and outliers, we divide each one of them into 3 subsets. The first is the Training Set and it contains the 60\% of the whole set. This data is the one that is used to obtain the regression parameters with the different algorithms. The second subset is the Validation Set, which contains a 20\% of the remaining data. The data in the Validation Set is used for comparing different configurations of the same algorithm. This is specially useful and recommended when dealing with neural networks \cite{aggarwal_neural_2018}, since it is usually needed to compare between different architectures and configurations. Finally, the last is the Test Set, that contains the remaining 20\% of the data. This set is used as new data to test the performance of the algorithms with unseen examples. This process is usually referred to as \textit{generalization}. The main purpose of this splitting of the data is for the algorithms to generalize \textit{well}, this is obtaining an equivalent performance when dealing with new data than when dealing with the Training Set.

Finally, the last step in the data preparation is the \textit{feature scaling}. Two of the regression algorithms that have been implemented, namely the gradient descent and the neural network, require having features with similar magnitudes, usually they accept up to a power of ten between features. A more detailed explanation for this necessity is detailed later in chapter \ref{ch:fitting} but the main reason is that having features with different orders of magnitude usually hinders the convergence of the algorithms. The feature scaling is performed by normalizing the features as

\begin{equation}
    \btheta_{i, train}^{norm} = \frac{\btheta_{i, train} - \bar{\btheta}_{i, train}}{\sigma_{i, train}}
\end{equation}

where $\btheta_{i, test}$ denotes the $i$-th feature in the Training Set and it has a measured mean $\bar{\btheta}_{i, test}$ and standard deviation $\sigma_{i, test}$. 

Once the parameters of the model have been obtained by the regression algorithm, it is important to normalize the data used for evaluation as well. These datasets, namely the Validation and Test sets, must be normalized with the mean and standard deviations obtained with the Training Set normalization in order to be consistent. Then we obtain the normalized datasets as:

\begin{equation}
    \btheta_{i, val}^{norm} = \frac{\btheta_{i, val} - \bar{\btheta}_{i, train}}{\sigma_{i, train}}
\end{equation}

\begin{equation}
    \btheta_{i, test}^{norm} = \frac{\btheta_{i, test} - \bar{\btheta}_{i, train}}{\sigma_{i, train}}
\end{equation}



%==================================================================================================================
\chapter{Regression algorithms}\label{ch:fitting}

\section{MATLAB fitting}\label{matlabfit}

The reality we live is extremely complicated, therefore it is crucial to define models in order to understand it better. Regression analysis is a statistical tool which enables to establish an existing relationship between an output variable and one or multiple input variables. Therefore, it helps to depict the causal effect of certain parameters upon other ones \cite{IntroToRegresAna}. Regression models have been long used in many fields, such as economics, medicine or chemistry, principally for purposes related to predict, forecast and estimate values between observed data points \cite{MathWorksRegression}.  

Throughout this project, regression models have been considered in order to estimate the distribution of the errors present in the aforementioned features. This section focuses on nonlinear regression of the mean and standard deviation (STD) of the Multipath Error (PRE) as a function of elevation and Carrier-to-Noise (CN0) by using the Curve Fitting Toolbox\textsuperscript{\tiny\texttrademark} software developed for MATLAB\textsuperscript{\tiny\textregistered}. The aim of using this estimation technique is to provide a first intuition about the error modelling of the data initially provided. In addition, this fitting is a tool used to provide a reference of the resulting error model in order to compare the performance with other methods. Thus, the algorithms implemented can be validated as it was assumed that the results must be improved using more advanced techniques, such as Gradient Descent (GD) or Neural Networks (NN), which are able to generate the error model automatically only providing the dataset. 

The main idea of the regression method explained in this section is to define a cost function which characterizes the relation between two or more variables, so it can be used to predict the output cost of a certain input value even if it is not include in the given dataset. There exist many techniques to accomplish this goal, nonetheless, we will focus on the Least Squares (LS) method, which will be detailed further on. The core of this algorithm consists in minimizing the sum squared of the errors between the observed data and the prediction made by our model for the same points. By squaring the errors, the algorithm penalizes the worst points, those that present larger errors given that the predicted value is much further from the observed point. As a result, we obtain a combination of parameters which minimizes the cost function, so it represents the best relation (fit) between the input and output variables in order to obtain the lowest errors. 

%define a cost function which depicts the existing relationship between two or multiple variables and allows us to predict the value of the output variable even for input values that are not contained in the dataset 

The Curve Fitting Toolbox\textsuperscript{\tiny\texttrademark} includes a set of functions and utilities which allow to perform a regression by fitting a curve or surface to the data through linear and nonlinear models, or custom equations \cite{MathWorksRegression}. In consequence, fitting requires to define a parametric model which establishes a relation between a set of training data and the output variable using a number of coefficients, so that the output continuous-valued parameter can be predicted. 

Fitting a curve or a surface mainly depends on the number of features available. Thus, the measurement model is build in accordance with the number of features. In our case, there are two possible options: selecting one or two features. As a result, the algorithm fits a curve when one feature is selected (univariate nonlinear regression), otherwise it fits a surface (multivariate regression).

As we mentioned before, this software uses the Least Squares (LS) regression method to fit the data, which consists in obtaining the best estimator by minimizing the sum of squared residuals. For this fitting, two different nonlinear measurements models are considered: polynomial and exponential. 

Given a general nonlinear function $h$ to represent the measurement model for one feature, such that
\begin{equation}
    y = h(x,p) + \epsilon
\end{equation}
where $y$ is the output variable, $x$ is the input variable, $p$ corresponds to the coefficients of the model and $\epsilon$ corresponds to the error. $h$ may be also referred to as \textit{hypothesis}. Consequently, from the general model, it is possible to go into detail with respect to the aforementioned nonlinear models. 

On one side, a polynomial measurement model for a single feature is assumed, such that
\begin{equation}
    y_k = \sum\limits_{i=i}^{n+1} p_{i}x^{n+1-i},  \;\;\mbox{for}\; k = 1,...,M
\end{equation}

where $n+1$ is the order of the polynomial, which indicates the number of coefficient to be fit, and $n$ is the degree of the polynomial, which gives the highest power of the predictor variable. In addition, $x = [x_1,...,x_N]$ is the set of N know inputs, $y = [y_1,...,y_M]$ is the set of M observations and $p = [p_1,...,p_N]$ corresponds to the coefficient estimates.

The degree of the polynomial and, thus, the number of coefficients to be estimated may be modified according the problem requirements or needs. 
An example of the resulting model may be the following
\begin{equation}
\begin{split}
    y_k &= p_{1}x^{2} + p_{2}x + p_3,    \;\;\mbox{for}\; k = 1,...,M
\end{split}
\end{equation}

However, the toolbox allows to fit curves up to polynomials of degree 9, if desired. 

The main advantages of polynomial fits include reasonable flexibility for data that is not too complicated, and they are linear, which simplifies the fitting process \cite{MathWorksRegression}. However, high-degree models can become unstable and diverge extensively outside the data range \cite{MathWorksRegression}. 
% Explaining why polynomial here?

On the other side, it is also presented the one-term exponential measurement model, such that 
\begin{equation}
    y_k = p_{1}e^{p_{2}x_{k}},  \;\;\mbox{for}\; k = 1,...,M
\end{equation}
%% Explain why using exponential, advantages? disadvantages?

After introducing the measurements models implemented for one feature, the measurements models for two features are presented hereunder.

The nonlinear $h$ can be expressed as a function of two variables:
\begin{equation}
    y = h(x_{1},x_{2},p) + \epsilon
\end{equation}
where $y$ is the output variable, $x_{1}$ and $x_{2}$ are the features, $p$ corresponds to the coefficients of the model and $\epsilon$ corresponds to the error. 

This model can also be further detailed considering the polynomial model. It is important to notice that, in this case, the exponential model is not available for surface fitting in the MATLAB toolbox used. In consequence, the estimation of the mean and STD with two features has been carried out has been carried out using only a polynomial hypothesis.

An example of a polynomial measurement model with two features can be as follows:
\begin{equation}
\begin{split}
    y_k &= p_{00} + p_{10}x_{1} + p_{01}x_{2} + p_{11}x_{1}x_{2} + p_{20}x^{2}_{1} + p_{02}x^{2}_{2},    \;\;\mbox{for}\; k = 1,...,M
\end{split}
\end{equation}

In this case, the toolbox allows to fit a surface using polynomials up to degree 5. 

Once the measurement model is defined, it is time to introduce the main core of the fitting algorithm, which is the nonlinear Least Squares method. As it has been previously stated, the goal of the LS is to minimize the sum of squared residuals $r_k$ (SSR), so that the coefficient estimates $\hat{p}$ are obtained.
\begin{equation}\label{eq:SSR}
    SSR(p) = \sum\limits_{k=1}^{M}{r}_{k}^{2} = \sum\limits_{k=1}^{M}(y_k - \hat{y}_k)^2 = \sum\limits_{k=1}^{M}(y_k - \sum\limits_{i=i}^{n+1} p_{i}x^{n+1-i})^2
\end{equation}

Equivalently, equation \eqref{eq:SSR} can be expressed in a matrix way:
\begin{equation}
    SSR(p) = \sum\limits_{k=1}^{M}(Y - x_{i}p)^2 = (Y - xp)^T(Y - xp)
\end{equation}

Equation \eqref{eq:SSR} represents the cost function of the model, which gives a good measure of how well the particular model fits the dataset \cite{MachLearnRefined}. In order to obtain the minimum $\hat{p}$, this function must be minimized, which is accomplished by applying differentiation and scaling techniques:
\begin{equation}
\begin{split}
    \left.-\frac{1}{2}\pdv{(SSR(p))}{p_{j}}\right|_{\substack{\hat{p}}} &= x^{T}(Y - xp) = 0
\end{split}
\end{equation}

The final expression of the estimates is as follows:
\begin{equation}\label{eq:estimatorLS}
\begin{split}
    \hat{p} = (\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}Y = \textbf{S}Y\\
\end{split}
\end{equation}

where $\textbf{X}$ is the matrix of features, $\textbf{W}$ corresponds to the weight matrix and $\textbf{S}$ is called generalized inverse or pseudoinverse.

As nonlinear models are more difficult to fit than the linear ones, an iterative approach is required to perform instead of simple matrix techniques \cite{MathWorksRegression}. So, it is necessary to provide an initial estimate of the coefficients, which is set to [0,0] in this case. Hence, the function produces a initial fitted curve for the current set of coefficients. Then, the software adjusts the coefficients and determine whether the fit improves. Finally, it iterates the process until the fit reaches the specified convergence criteria \cite{MathWorksRegression}. 

In addition to the nonlinear LS, we have also implemented a nonlinear Weighted Least Squares (WLS). This variant of the LS is specially useful when the assumption such that the output data has constant variance is no longer fulfilled. Therefore, the WLS is a powerful tool to improve the fit, which consists in including additional weights in the fitting process in order to give more importance (more weight) to the high-quality data, which will have a greater impact on the final coefficient estimates. 

Due to the weights, defined as $w_{k}$, the expression of the SSR in equation \eqref{eq:SSR} results in
\begin{equation}\label{eq:SSR_2}
    SSR(p) = \sum\limits_{k=1}^{M}w_{k}{r}_{k}^{2} = \sum\limits_{k=1}^{M}w_{k}(y_k - \hat{y}_k)^2
\end{equation}

Accordingly, 
\begin{equation}
    SSR(p) = (Y - xp)^T\textbf{W}(Y - xp)
\end{equation} 

where $\textbf{W} = \frac{1}{h(x,p)}\textbf{I}$, considering $\textbf{I}$ an identity matrix. 

So, the formulation of the estimates is modified such that
\begin{equation}\label{eq:estimatorWLS}
\begin{split}
    \hat{p} = (\textbf{X}^{T}\textbf{W}\textbf{X})^{-1}\textbf{X}^{T}\textbf{W}Y\\
\end{split}
\end{equation}

\subsection{Architecture of our curve fitting}

Once the fundamentals of the curve fitting are understood, we can proceed to introduce the structure of the algorithm implemented in order to clarify the steps carried out for the present regression method.

First of all, as it has been mentioned previously, the fitting is conducted in order to obtain the coefficient estimates of the mean ($\mu$) and STD ($\sigma$) of the PRE. To achieve this objective, it is important to differentiate both types fittings. 

Regarding to the mean fitting, the input parameters of the function correspond to the given features and the observed data points. In this case, the features are the elevation and CN0 and the observed data corresponds to the PRE. The magnitude of the PRE depends on many factors and, among them, the aforementioned ones are present. Hence, both inputs are necessary to perform the LS regression and, therefore, obtaining the output, which corresponds to the estimate of the variable being measured: the mean of the PRE.  

As a quick reminder of the previous chapter \ref{ch:data_analysis}, at this point, the data has already been processed in order to discard invalid or undesired values. 

In this sense, the procedure followed for the computation of the mean fitting can be resumed with the following steps:
\begin{enumerate}
    \item Define fitting options,
    \item Introduce features and observed data points,  
    \item Apply curve fitting function to estimate the coefficients of the model,
    \item Evaluate the regression model with the input data and the coefficients estimates,
    \item Compute Root Mean Squared Error (RMSE). 
\end{enumerate}

Regarding the RMSE, it represents a measure of the error made by the model in predicting the observed data. The RMSE of predicted values is computed as follows:

\begin{equation}
    RMSE = \sqrt{\frac{\sum\limits_{k=1}^{M}{r}_{k}^{2}}{M}} = \sqrt{\frac{\sum\limits_{k=1}^{M}{r}_{k}^{2}}{M}} = \sqrt{\frac{\sum\limits_{k=1}^{M}(y_k - \hat{y}_k)^2}{M}} 
\end{equation}

This computation is equivalent for the mean and the STD estimation, as well as for the WLS. The only consideration is that the observed values $y$ and predicted ones $\hat{y}$ represent different parameters in each case.  
 
Hence, RMSE is a useful tool to get a measure of the spread of the $y$ values about the predicted $\hat{y}$ values \cite{StanfordRMSE}. As the effect of each error on the RMSE is proportional to the size of the squared error, then larger errors have a disproportional impact on the RMSE, leading to extremely large errors. As a result, RMSE is very sensitive to outliers. In this case, the RMSE is a parameter used to analyze and compare the performance of the different regression models implemented in this project.   

On the other hand, the fitting of the STD carries out the estimation of the error of the observed variables, in this case the PRE, with respect to the features provided. This particularity requires a previous step in comparison with the mean fitting. As the regression is applied to the error of the observed parameter instead of the total value, then the residuals are additionally computed prior to perform the fitting.  

So, the input variable or vector corresponds to:
\begin{equation}\label{eq:inputstd}
\begin{split}
    r_{k,\sigma} = |y_k - \hat{y}_{k,\mu}| = |y_k - h(x_{k},\hat{p}_{k,\mu})| = |y_k - x_{k}p_{k,\mu}|\\
\end{split}
\end{equation}

Once the computation is made according to equation \eqref{eq:inputstd}, the same steps as previously stated for the mean fitting are followed. 

Finally, this procedure is repeated for the nonlinear WLS regression method, in which the computation of the weights must be taken into consideration before the fitting for both mean and STD. 

% The main reason behind the decision to select a polynomial model is the distribution of the data provided. According to the results presented in chapter \ref{ch:data_analysis}, 

\section{Gradient descent}

Gradient descent is an optimization algorithm which has become very popular in the last years as it is a crucial algorithm for training machine learning models. It is present in most Artificial Intelligence (AI) systems that are being developed these days. However, its origin dates back to the 19th century, when it was developed by Lous Augustin Cauchy in his \textit{Compte Rendu à l’Académie des Sciences} of October 18, 1847 \cite{cauchy}.

First of all, it is important to recall the main idea behind the regression technique detailed in section \ref{matlabfit}. The objective was to minimize a given cost function, also known as hypothesis or simply $h$, in order to find the parameters such that the value of the cost function evaluated at those parameters corresponds to the global minima of the function. As the cost function measures the accuracy of the hypothesis of our model to predict the output variable, then finding the minimum of $h$ implicates that the error made by the prediction model is the lowest. Hence, it is very interesting to develop advanced techniques in order to optimize this minimization problem. 

Before expatiating upon GD, it is necessary to introduce some mathematical concepts related to functions. In particular, some important properties of a convex function and their implications on GD. 

The main characteristic of a convex function is that there exists a stationary point at which the function's derivative is zero, assuming that this function is differentiable. In a very informal way, at this point, the slope of function is zero, so it neither increases or decreases. In addition, there is another property which states that, in case of finding a minimium point in the function, it is in fact a global minimum. That is to say that there are no other points located below that this one on the function. 

However, the algorithm may encounter a type of functions, also known as non-convex, which presents other minimums (local minima) besides the global minima, as it is depicted in figure \ref{fig:Minimas}.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\textwidth]{fitting/Local-and-Global-minima_W640.jpg}
    \caption{Local and Global minima}
    \label{fig:Minimas}
\end{figure}

In this case, the algorithm may have to deal with multiple minimum points and, therefore, multiple equations to solve, which correspond to a derivative equal to zero for each point. Besides the local minima, there also other points which can fulfill the same premise, so the system of equations can become extremely large. This can result in a serious problem for optimization algorithms, not only in terms of computational efficiency, but also because they can get stuck in a local minima and may never find the global one. In consequence, it is essential to find a solution for the non-convex functions. 

The GD method aims to solve this problem by presenting an intuitive reasoning in which it takes advantage of the information provided by the computation of the derivative (the slope) in order to find the local minimums of the function. The idea is to start by evaluating the slope at the current position in order to determine the greatest slope, then move forward in that direction in which the slope is maximum according to the current position and, finally, stop at a new position and evaluate again. This is an iterative process which continues until convergence is reached. 

In a more formal way, the GD computes the partial derivatives of each parameter at a specific point and the values obtained will provide information of the slope in the corresponding axis of each parameter. The set of all the partial derivatives result in a vector that indicates the direction in which the slope increases. This vector is referred to as the gradient $\nabla J$. 

Considering first a simple example using two input parameters $\btheta = [\theta_{0}, \theta_{1}]^T$ and the error $e$ as the output, then the gradient expression is as follows:
\begin{equation}
    \nabla J(\theta_{0},\theta_{1}) = \pdv{}{\theta} J(\theta_{0},\theta_{1}) = \begin{bmatrix}
                                                                                \pdv{e}{\theta_{0}} &                                 \pdv{e}{\theta_{1}}\\
                                                                                \end{bmatrix}^T
\end{equation}
where $J(\theta_{0},\theta_{1})$ refers to the cost function as a function of the parameters $\theta_{0}$ and $\theta_{1}$ previously defined. As the objective is to descent to the minimum point, then $\nabla J$ must be considered negative. It is also important to remark that the training parameters are usually initially set at a random point before starting the iteration process.

So, the resulting the computation of the parameters is
\begin{equation}\label{eq:gdeq}
\begin{split}
    &\mbox{repeat until convergence}\{\\
    &\theta \coloneqq \theta - \alpha \nabla J(\theta_{0},\theta_{1})\\
&\}
\end{split}
\end{equation}

where $\alpha$ corresponds to the learning rate. This factor measures the magnitude of the gradient's impact on the parameters update at each iteration. It defines the longitude of the step the algorithm takes to advance in a certain direction at each iteration. Therefore, it is crucial to set a proper value, otherwise, we might have to face the following situations:
\begin{itemize}
    \item  If $\alpha$ is too low, then the GD can become slow and take more time to converge.
    \item  If $\alpha$ is too high, then the GD can overshoot the minimum and may fail to converge or even diverge.  
\end{itemize}

Nevertheless, an alleviating aspect regarding $\alpha$ is that there is no need to decrease the factor over time. By setting a constant learning rate, we can notice that when the algorithm approaches to a minimum, the gradient becomes smaller as it tends to 0. So, the step taken becomes automatically smaller. Because of that, the right initilization of $\theta$ will determine which minimum the algorithm reaches. 

As we have mentioned, this process is computed iteratively until the algorithm reaches a location where the variation of the cost is low, so the slope is close to zero and it can be assumed that a minimum is found. From equation \eqref{eq:gdeq}, it can be observed that all the parameters must be updated simultaneously, given that $\nabla J$ depends on all of them. Not applying this practice could lead to large errors due to the mixed use of parameters computed in different iterations. 

Focusing on our case, the GD algorithm implemented in this project takes at least two features into consideration, so the following mathematical expression are based on a multivariate Gradient Descent.  

Given a $n$ number of features, first it is necessary to define the hypothesis form. In our case, it is expressed as a polynomial model of degree $m$. To obtain the terms of our polynomial hypothesis, a function called \texttt{mapFeatures}  has been developed to perform combination samplings with repetition according to $n$ and $m$. 

This can be better understood with an example. For instance, given 2 features ($n$ = 2) and a polynomial model of degree 2 ($m$ = 2), then the final expression of the hypothesis is:
\begin{equation}\label{eq:gdH}
\begin{split}
    h_{\theta}(x) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x^{2}_{1} + \theta_{4}x_{1}x_{2} + \theta_{5}x^{2}_{2}\\
\end{split}
\end{equation}

From a matrix approach, this is equivalent to express
\begin{equation}\label{eq:gdHmatrix}
\begin{split}
    h_{\theta} = \theta^{T}x\\
\end{split}
\end{equation}

In this formulation, $x = [x_0,x_1, x_2,...x_n]^{T}$ corresponds to the input features contained in a vector of dimensions $(n+1)x1$ and $\theta$ are the parameters to be estimated contained in a column vector of $(n+1)x1$. $x_0^{(i)}$ is equal to 0 for all the training examples of the set. In consequence, as the number of features and degree of the polynomial increases, more complex hypothesis are obtained.  

Once the hypothesis is established, the cost function is defined as
\begin{equation}\label{eq:gdcost}
\begin{split}
    J(\theta) = \frac{\sum\limits_{i=1}^{m} ((\sum\limits_{j=1}^{n}\theta_{j}x_{j}^{(i)}) - y^{(i)})^{2}}{2m} = \frac{\sum\limits_{i=1}^{m}(\theta^{T}x^{(i)} - y^{(i)})^{2}}{2m}\\
\end{split}
\end{equation}

where $m$ is the number of training examples, $n$ is the number of features, $x^{(i)}$ is the input feature of the $i$th training example and $y^{(i)}$ is the observed point for that training example. 

Equation \eqref{eq:gdcost} can also be formulated using matrices such that
\begin{equation}\label{eq:gdcostmatrix}
\begin{split}
    J(\theta) = \frac{(\textbf{X}\theta - y)^{T}(\textbf{X}\theta - y)}{2m}\\
\end{split}
\end{equation}
where $\textbf{X}$ is the matrix containing all the $x$-term combinations of the hypothesis for every training example.

After that, we can proceed to compute the parameters
\begin{equation}\label{eq:gdtheta}
\begin{split}
    &\mbox{repeat until convergence}\{\\
    &\theta_{j} \coloneqq \theta_{j} - \frac{\alpha}{m} \sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)}) -  y^{(i)})x_{j}^{(i)}\\
&\}
\end{split}
\end{equation}
where $x_{j}^{(i)}$ is the value of the feature $j$ in the $i$-th training example and the rest of the variables remain the aforesaid definition. 

For the matrix expression of $\theta$, we obtain that
\begin{equation}\label{eq:gdthetaMatrix}
\begin{split}
    \theta_{j} = \theta_{j} - \frac{\alpha}{m}\textbf{X}^T(\textbf{X}\theta - y)\\
\end{split}
\end{equation}

The term $\textbf{X}^T(\textbf{X}\theta - y)$ corresponds to the error between the observed data and our prediction.

Regarding the implementation of the GD, there are many different ways. However, we decided to implement a \textit{Batch} Gradient Descent, which consists in taking the full set of training examples at each iteration for computational efficiency. This practice has been applied in the code by implementing the matrix formulation previously presented in equations \eqref{eq:gdHmatrix}, \eqref{eq:gdcostmatrix} and \eqref{eq:gdthetaMatrix}.

Another fundamental aspect which must be taken into account during the implementation is the feature scaling. In terms of unconstrained optimization, a problem is said to be \textit{poorly scaled} if changes to a particular feature, for example $x_{1}$, in a certain direction produce much larger errors in $h_\theta(x)$ than changes to another features, such as $x_{2}$ \cite{numericalopt}. So, poor scaling is due to high sensitivity of the hypothesis to small changes in certain features and low sensitivity to variations in other features. 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.6\textwidth]{fitting/PoorScaling_GradDesc.PNG}
    \caption{Poorly scaled and well scaled problems, and performance of the gradient descent direction \cite{numericalopt}.}
    \label{fig:scaling_gd}
\end{figure}

Figure \ref{fig:scaling_gd} presents two different contour plots for the cost function, where the top one is poorly scaled and the bottom one is well scaled. As it can be noticed in the top one, the gradient descent does not work properly. As it calculates the Euclidean distance between points, features with wider range of values will will have a greater impact on the distance, which will lead the cost functions to be dominated by these features. In consequence, we obtain this type of ellipsoid-shaped contours, where the algorithm would oscillate several times back and forth, taking a long time to find the minimum and, thus, converge. On the other side, observing the bottom plot, we can see that the well-scaled problem presents a bowl-shaped contour plot, which lets the GD algorithm to work perform much better and work efficiently.  

As we have seen, GD algorithm is very sensitive to poor scaling \cite{numericalopt}, so the idea behind \textit{feature scaling} is to define all the features involved in our optimization problem in a similar scale in order to achieve a more balanced solution. The method is based on applying normalization and standardization techniques to the range of the features. This process has been already detailed in section \ref{DataPrep}. 

Once the algorithm is implemented, it is time to validated it and evaluate its performance. To accomplish this, a common practice is to plot the cost function $J(\theta)$ versus the number of iterations. Displaying the evolution of the cost function over time can help to see if the algorithm is working properly and check the convergence of the GD. The number of iterations required to converge can vary depending on the nature of the problem, however, convergence must be met. Therefore, if $J(\theta)$ decreases over iterations, then we can ensure a good performance of the GD. Otherwise, if $J(\theta)$ increases or presents an irregular shape, the GD does not works correctly. Sometimes, this problem can be easily solved by setting a smaller $\alpha$. The most common practice when implementing the algorithm is to try different values of $\alpha$ in order to see which results in a better performance. 

Finally, there is a last point which is important to remark. So far, this project has described two different approaches to solve the same regression problem. The first method, detailed in section \ref{matlabfit}, presented a solution that requires the computation of equation \eqref{eq:estimatorLS}, also referred to as normal equation. The advantage of implementing this technique is that it can be solve in just one step, as it is not iterative, and there is no need to apply feature scaling and $\alpha$. However, the computation of $(\textbf{X}^{T}X)^{-1}$ may be very complex and slow, as it requires a large computational cost. Besides that, it can also occur that $\textbf{X}^{T}X$ may be noninvertible. This might be due to redundant features, which are very close related, or too many features. In this last case, regularization can be applied. 

On the other side, the method of GD explained throughout this section presents some interesting advantages with respect to the previously one. Even though it is necessary to set a learning rate and feature scaling, the method requires less computational effort to be executed iteratively than to compute the inverse. Therefore, the execution time will be lower and it will converge faster. Additionally, as long as the hypothesis fulfills the properties of a convex or non-convex function, the algorithm will always find a minimum and, consequently, a solution. 


\section{Neural Network}
% Introduction
The concept of Artificial Neural Networks, also known just as neural networks, was first introduced in the early 1940s by McCullock and Pitts, who theorized about a computational model for artificial intelligence which was inspired on the neural networks in the brain \cite{mcculloch_logical_1943}. Later, in 1958, Rosenblatt developed the perceptron which can be considered as the equivalent of a neuron in Artificial Neural Networks \cite{rosenblatt_perceptron_1958}. The perceptron allowed to learn from a set of example data and later perform binary classification of new examples. In 1969 it was demonstrated that perceptrons were incapable of performing non-linear classifications such as performing the logical XOR operation \cite{minsky69perceptrons}. The development of the backpropagation algorithm in 1975 renewed the interest in neural networks because it allowed to train multiple-layer perceptrons \cite{werbos}. These were networks of various neurons that allowed the learning of non-linear functions. Even then, neural networks were rarely used due to the high computational complexity and the large amounts of data that they required. An increase in transistor count and a latter raise in the use of GPUs have bring back to life the neural networks, also the Internet explosion allowed a much easier collection of large amounts of data, which are needed to train neural networks \cite{aggarwal_neural_2018}.

Lately, neural networks have outperformed other regression algorithms in many complex problems and have also empowered new and unforeseen applications such as vehicle control \cite{BojarskiTDFFGJM16}, image recognition \cite{hijazi2015} or medical diagnosis \cite{AMATO201347}.

\subsection{Model representation}
The basic element in a neural network is called neuron. A neuron is nothing more that an element that takes some weighted inputs, adds them to obtain $z$ and maps the result to another value, $g(z)$, which is the output. The function $g(z)$ is called \textit{activation function} and it is the main characteristic of the neuron. The behavior of a neuron is expressed as

\begin{equation}
    g(z) = g(\btheta^T\boldsymbol{x})
\end{equation}

where $\x = [x_0, x_1, x_2, ..., x_n]^T$ are the input values $x_0 = 1$, and $\btheta = [\theta_0, \theta_1, \theta_2, ..., \theta_n]$ are the weights assigned to these input values. The idea of this model is that, given the correct weights to the input values and the right activation function, it will estimate a value $y^{(i)}$ which depends on the input $\boldsymbol{x^{(i)}}$ as $h_\theta(\boldsymbol{x^{(i)}})$. This neuron can be represented with the diagram in Fig. \ref{fig:neuron_diag}. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fitting/neuron_diagram.png}
    \caption{Diagram representation of a neuron}
    \label{fig:neuron_diag}
\end{figure}

In order to find the best weights $\btheta$, the error function is defined as we did in the previous section and it can be minimized with the gradient descent method. As it has been previously mentioned, a single neuron does not work well for complex non-linear processes. To be able to make predictions in those cases, a more complex system is required. 

Neural networks are built by using the outputs of various neurons as the inputs for other neurons, then building layers. The first one is called \textit{input layer}, the last one, \textit{output layer} and the layers in between are usually called \textit{hidden layers}. Usually, a neural network with a high number of layers will be able to perform more complex predictions but it will also take more time to train.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fitting/neural_network_example.png}
    \caption{Example of a neural network}
    \label{fig:neural_network_example}
\end{figure}

An example of a neural network is shown in Fig. \ref{fig:neural_network_example}. In this diagram, $a_i^{(j)}$ denotes the result of evaluating the activation function at the unit $i$ of the layer $j$. It is important to remark that $x_0$ and the activation values $a_0^{(j)}$, which are known as bias, are always set to 1. This is the reason why these nodes do not have any input. Also, we define $\bTheta^{(j)}$ as the matrix of weights controlling the function mapping from layer $j$ to $(j+1)$. The procedure to estimate $h_\Theta(\x)$, also known as forward propagation, consists on computing the activation $a_i^{(j)}$ at each one of the units in the neural network, starting from the input layer and moving forward to the output layer. This process is shown below for the example in the diagram of Fig. \ref{fig:neural_network_example}.

\begin{enumerate}
    \item Activation of layer 2
    \begin{equation}
    \begin{split}
        a_1^{(2)} &= g(z_1^{(2)}) = g(\bTheta_{10}^{(1)} + \bTheta_{11}^{(1)} x_1 + \bTheta_{12}^{(1)} x_2) \\
        a_2^{(2)} &= g(z_2^{(2)}) = g(\bTheta_{20}^{(1)} + \bTheta_{21}^{(1)} x_1 + \bTheta_{22}^{(1)} x_2) \\
        a_3^{(2)} &= g(z_3^{(2)}) = g(\bTheta_{30}^{(1)} + \bTheta_{31}^{(1)} x_1 + \bTheta_{32}^{(1)} x_2)
    \end{split}
    \end{equation}
    \item Activation of layer 3
    \begin{equation}
    \begin{split}
        a_1^{(3)} &= g(z_1^{(3)}) = g(\bTheta_{10}^{(2)} + \bTheta_{11}^{(2)} a_1^{(2)} + \bTheta_{12}^{(2)} a_2^{(2)} + \bTheta_{13}^{(2)} a_3^{(2)}) \\
        a_2^{(3)} &= g(z_2^{(3)}) = g(\bTheta_{20}^{(2)} + \bTheta_{21}^{(2)} a_1^{(2)} + \bTheta_{22}^{(2)} a_2^{(2)} + \bTheta_{23}^{(2)} a_3^{(2)}) \\
        a_3^{(3)} &= g(z_3^{(3)}) = g(\bTheta_{30}^{(2)} + \bTheta_{31}^{(2)} a_1^{(2)} + \bTheta_{32}^{(2)} a_2^{(2)} + \bTheta_{33}^{(2)} a_3^{(2)})
    \end{split}
    \end{equation}
    \item Activation of layer 3: prediction
    \begin{equation}\label{eq:nn_cost_function}
        h_\Theta(\x) = a_1^{(3)} = g(z_1^{(4)}) = g(\bTheta_{10}^{(3)} + \bTheta_{11}^{(3)} a_1^{(3)} + \bTheta_{12}^{(3)} a_2^{(3)} + \bTheta_{13}^{(3)} a_3^{(3)})
    \end{equation}
\end{enumerate}

The architecture of a neural network is mainly defined by the number of layers, the number of units in each layer and the activation functions that are used. The number of units of the input and output layers is usually restricted by the characteristics and goals of the application at hand. The number of input nodes is determined by the number of features that are used, since this layer is indeed used to feed the network with data. The number of output units can be larger than one and it is chosen depending on the type of problem that is confronted. For example, a classification problem requires as many output units as classes, so that the output of each unit $k$ represents the probability of the input example to pertain to the class $k$. 

The size of the hidden layers, both in terms of number of layers and number of units per layer, is generally a difficult part in the design of a neural network since there is no direct rule to decide among the different possibilities. Therefore, these are usually chosen by trial and error procedures. On the one hand, having too small hidden layers may lead to underfitting or even the inability of the algorithm to converge. On the other hand, having too large and/or too many hidden layers will increase the computation complexity and may also lead to overfitting \cite{aggarwal_neural_2018}. Nonetheless, a big majority of problems can be faced using a network with one hidden layer \cite{aggarwal_neural_2018}. Actually, it has been shown that a single hidden layer of nonlienar units and a single linear output layer can compute almost any "reasonable" function \cite{hornik_multilayer_1989}. Although this theoretical claim is not always easy to apply because it may require a huge amount of features.

The choice of the activation cost function is a critical decision in the design of a neural network. Any network with all the activation functions being linear will provide the same results as a Least Squares. In order to obtain more complex behavior, some unlinearities need to be added. To do so, many options of activation functions are proposed. The performanece of these different activation functions also depends on the type of problem that is being faced, but it usually requires some trial and error as well. Next, some examples of activation functions are shown.

\begin{itemize}
    \item Linear function
    \begin{equation}\label{eq:act_linear}
        g(z) = z
    \end{equation}
    \item Sigmoid function
    \begin{equation}\label{eq:act_sigmoid}
        g(z) = \frac{1}{1+e^{-z}}
    \end{equation}
    \item Tanh function
    \begin{equation}\label{eq:act_tanh}
        g(z) = \frac{e^{2z} - 1}{e^{2z} + 1}
    \end{equation}
    \item Rectifier Linear Unit (ReLU)
    \begin{equation}\label{eq:act_relu}
        g(z) = \max(0, z)
    \end{equation}
\end{itemize}



\subsection{Training a neural network}
Training a neural network basically consists in finding the weights for the $L$ layers of that network, $\bTheta = \{\bTheta^{(1)}, \bTheta^{(2)}, ..., \bTheta^{(L)}\}$, that minimize the cost function of the neural network, which describes the error between the actual measurements and the predictions. This is

\begin{equation}
    \min_{\bTheta} J(\bTheta) = \min_{\bTheta} \frac{1}{2m} \left(\sum_{i=1}^{m} \left(y^{(i)} - h_\Theta (\x^{(i)}) \right)^2 \right) + \frac{\lambda}{2m} \sum_{l=1}^{L} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\bTheta_{j,i}^{(l)})^2
\end{equation}

where $m$ is the number of examples in the data set, $\lambda$ denotes the regularization term, and $s_l$ is the number of units of the layer $l$.

To obtain this minimum in $J(\bTheta)$ we use the gradient descent. The problem that we encounter is that, given the very high complexity of this cost function, we virtually cannot compute its gradient. This is where the backpropagation algorithm appears. This method allows to obtain the equivalent of the gradient for a neural network in a much more efficient way.

This algorithm starts with a random initialization of the parameters $\bTheta$. Then, it performs forward propagation with an example $(\x^{(i)}, y^{(i)})$ to obtain an estimation $h_\Theta(\x^{(i)})$. Once the estimation is obtained, the error of $h_\Theta(\x^{(i)})$ with respect to $y^{(i)}$ is obtained. This error is then "propagated" back in the network, so that we obtain a representation of the contribution of each node to the final error. This process is then repeated for each one of the examples given in the training set. The exact procedure of this algorithm is detailed next.

Given a training set $\set{(\x^{(1)}, y^{(1)}), (\x^{(2)}, y^{(2)}), ... (\x^{(m)}, y^{(m)})}$

\begin{enumerate}
    \item Set $\bDelta_{ij}^{(l)} = 0$ , $\forall l, i, j$
    \item Randomly initialize $\bTheta = \{\bTheta^{(1)}, \bTheta^{(2)}, ..., \bTheta^{(L)}\}$
    \item For $i$ from 1 to m:
    \begin{enumerate}[i.]
        \item Perform forward propagation to obtain $\ba ^{(l)}$ for $l=2, 3, ..., L$
        \item Calculate $\delta^{(L)} = a^{(L)} - y$
        \item For l from $(L-1)$ to 2:
        \begin{enumerate}[a.]
            \item $\bdelta^{(l)} = (\bTheta^{(l)})^T\bdelta^{(l+1)} \odot g'(\bz^{(l)})$\footnote{The symbol $\odot$ denotes the Hadamard or element-wise product}
        \end{enumerate}
        \item Update $\bDelta^{(l)}$ as $\bDelta^{(l)} := \bDelta^{(l)} + \bdelta^{(l+1)}(\ba^{(l)})^T$
    \end{enumerate}
    \item End for
    \item Calculate $\bD_{ij}^{(l)} = \frac{1}{m} \bDelta_{ij}^{(l)}$ if $j = 0$
    \item Calculate $\bD_{ij}^{(l)} = \frac{1}{m} \bDelta_{ij}^{(l)} + \lambda \bTheta_{ij}^{(l)}$ if $j \neq 0$
\end{enumerate}

The final result that we obtain after the backpropagation algorithm is equivalent to the gradient of the cost function of the neural network

\begin{equation}
    \bD_{ij}^{(l)} = \pdv{}{\bTheta_{ij}^{(l)}} J(\bTheta)
\end{equation}

% Overfit / Underfit (here?)

\subsection{Architecture of our neural network}
Given the nature of the neural networks, there exist an infinity of possibilities when trying to decide the architecture of a network. Nonetheless, it is usually a good practice to start with a simple design and then, if needed, move to more complex configurations. In our case we started with one hidden layer which has the same number of units as the input layer. This design is widely recommended as it has been found to perform well in a large variety of cases. Also, since we are dealing with a linear regression problem we need to have only one unit in the output layer.

The other defining characteristic of a neural network is the activation function that is used. On the one hand, we decided to use a ReLU activation function for the units in the hidden layer. This activation function has shown very good performance in linear regression problems. On the other hand, the output layer implements a linear activation function, because the possible output values that we can have for our problem are not bounded.

%==================================================================================================================

\chapter{Results} \label{ch:results}

%=================================================================================================================================================

\chapter{Conclusions}\label{ch:conclusions}
In the last years, a better estimation of the pseudorange errors and an increase in the use of GNSS on urban environments has raised the interest in multipath prediction. Due to its nature, this source of error is usually really difficult to estimate as it depends a lot on the obstacles that a signal may encounter. 

Neural networks are a type of Machine Learning algorithms that have been showing very good performance in various diverse fields during the last two decades. Nonetheless, they have not been very used in GNSS applications. In this project, we have implemented a neural network that intends to estimate the impact of multipath error in GNSS pseudoranges from a set of features that could be obtained in real-time from a vehicle with the right equipment.

% ...


\clearpage
\printbibliography

\end{document}